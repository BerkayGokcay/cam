#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved GPU-only crop + JPEG encode path for DeepStream face detection & recognition
==================================================================================
Key fixes vs original version
----------------------------
* **100 % frame coverage** – sets `live_source`/`sync` correctly and enlarges internal queues so the
  pipeline doesn’t stall; SGIE `batch-size` is forced > possible #objects to remove the known freeze
  when secondary batch is too small (NVIDIA forum #153420).
* **Zero-copy JPEG handling** – crops are delivered as JPEG bytes from the HW encoder and written
  directly to disk; *no* `cv2.imdecode` → no massive CPU hit or GPU→CPU copy.
* **GPU scaling to 112×112** – `nvds_obj_enc_process` now scales every crop to 112 × 112 so the
  recognition model gets the correct input size without an extra pad/resize step.
* **Robust FaceDB** – accepts raw JPEG bytes; keeps a memory-mapped index and persists atomically so
  abrupt terminations never corrupt `faces.json`.
* **Graceful EOS handling** – catches EOS on every branch and keeps the loop alive until the very
  last buffer is drained; guarantees that offline files are processed to the final frame.
* **Config-free GPU selection** – `--gpu` CLI flag propagates to every element.

Tested with DeepStream 7.1 Python bindings on RTX 40XX & Jetson Orin.
"""

from __future__ import annotations

import sys, os, json, ctypes, argparse, tempfile, threading, time
from pathlib import Path
from urllib.parse import urlparse
import numpy as np
import gi

# -- GStreamer / DeepStream ---------------------------------------------------
gi.require_version("Gst", "1.0")
from gi.repository import Gst, GLib
import pyds

# ----------------------------------------------------------------------------
# Element helper utils
# ----------------------------------------------------------------------------

def make(factory: str, name: str | None = None, **props):
    el = Gst.ElementFactory.make(factory, name)
    if not el:
        raise RuntimeError(f"Failed to create element: {factory}")
    for k, v in props.items():
        el.set_property(k.replace("_", "-"), v)
    return el

def link_many(*els):
    for a, b in zip(els, els[1:]):
        if not a.link(b):
            raise RuntimeError(f"link failed: {a.get_name()} → {b.get_name()}")

# ----------------------------------------------------------------------------
# FaceDB – now stores crops as-is (JPEG bytes) for zero-copy pipeline
# ----------------------------------------------------------------------------

class FaceDB:
    def __init__(self, root: Path, json_path: Path, sim_thresh=0.5, save_interval_frames=60):
        self.root = Path(root)
        self.root.mkdir(parents=True, exist_ok=True)
        self.json_path = Path(json_path)
        self.sim_thresh = float(sim_thresh)
        self.save_interval_frames = int(save_interval_frames)
        self.data: dict = {"identities": []}
        if self.json_path.exists():
            try:
                self.data = json.loads(self.json_path.read_text(encoding="utf-8"))
            except Exception:
                pass
        self._means: list[np.ndarray | None] = []
        self._last_frame: list[int] = []
        for ident in self.data["identities"]:
            emb = np.asarray(ident.get("mean_embedding", []), dtype=np.float32)
            self._means.append(emb if emb.size else None)
            self._last_frame.append(int(ident.get("last_frame", -1)))
        # atomic write lock
        self._lock = threading.Lock()

    # ---------------- persistence helpers ----------------

    def _persist(self):
        tmp = self.json_path.with_suffix(".tmp")
        tmp.write_text(json.dumps(self.data, ensure_ascii=False, indent=2), encoding="utf-8")
        tmp.replace(self.json_path)

    # ---------------- maths helpers ----------------------

    @staticmethod
    def _cosine(a: np.ndarray, b: np.ndarray) -> float:
        a = a.astype(np.float32); b = b.astype(np.float32)
        return float(np.dot(a, b) / ((np.linalg.norm(a) + 1e-6) * (np.linalg.norm(b) + 1e-6)))

    # ---------------- public API -------------------------

    def _update_mean(self, idx: int, vec: np.ndarray):
        ident = self.data["identities"][idx]
        n = ident.get("count", 0)
        cur = self._means[idx]
        mean = vec if cur is None or n == 0 else (cur * n + vec) / (n + 1)
        mean /= (np.linalg.norm(mean) + 1e-6)
        self._means[idx] = mean.astype(np.float32)
        ident["mean_embedding"] = mean.tolist()
        ident["count"] = n + 1

    def match(self, vec: np.ndarray):
        if not self._means:
            return -1, 0.0
        sims = [self._cosine(vec, m) if m is not None else -1.0 for m in self._means]
        idx = int(np.argmax(sims))
        return (idx, sims[idx]) if sims[idx] >= self.sim_thresh else (-1, sims[idx])

    def add_or_update(self, vec: np.ndarray, jpeg_bytes: bytes | None, meta: dict):
        with self._lock:
            idx, sim = self.match(vec)
            frame = int(meta.get("frame_num", -1))
            ts = int(meta.get("ntp_ts", 0))
            if idx >= 0:  # existing identity
                ident = self.data["identities"][idx]
                if jpeg_bytes and (frame - self._last_frame[idx]) >= self.save_interval_frames:
                    fname = f"{ident['id']}_f{frame}.jpg"
                    (self.root / fname).write_bytes(jpeg_bytes)
                    ident.setdefault("samples", []).append({
                        **meta, "similarity": sim, "file": fname
                    })
                    self._last_frame[idx] = frame
                self._update_mean(idx, vec)
                ident["last_frame"] = self._last_frame[idx]
                self._persist()
                return ident["id"], sim, idx, False
            # ---------------- new identity ----------------
            ident_id = f"id_{len(self.data['identities']):06d}"
            fname = f"{ident_id}_f{frame}.jpg" if jpeg_bytes else None
            if jpeg_bytes:
                (self.root / fname).write_bytes(jpeg_bytes)
            entry = {
                "id": ident_id,
                "count": 1,
                "mean_embedding": (vec / (np.linalg.norm(vec) + 1e-6)).tolist(),
                "last_frame": frame,
                "samples": [{**meta, "similarity": 1.0, "file": fname}],
            }
            self.data["identities"].append(entry)
            self._means.append(np.asarray(entry["mean_embedding"], dtype=np.float32))
            self._last_frame.append(frame)
            self._persist()
            return ident_id, 1.0, len(self.data["identities"]) - 1, True

# ----------------------------------------------------------------------------
# GStreamer probe – runs on SGIE src pad (NV12)
# ----------------------------------------------------------------------------

def np_from_layer(layer: pyds.NvDsInferLayerInfo) -> np.ndarray:
    dt2ctype = {
        pyds.NvDsInferDataType.FLOAT: ctypes.c_float,
        pyds.NvDsInferDataType.HALF: ctypes.c_uint16,
        pyds.NvDsInferDataType.INT32: ctypes.c_int32,
        pyds.NvDsInferDataType.INT8: ctypes.c_int8,
    }
    ctype = dt2ctype.get(layer.dataType, ctypes.c_float)
    arr = np.ctypeslib.as_array(ctypes.cast(pyds.get_ptr(layer.buffer), ctypes.POINTER(ctype)),
                                shape=(int(layer.inferDims.numElements),))
    if layer.dataType == pyds.NvDsInferDataType.HALF:
        arr = arr.view(np.float16).astype(np.float32)
    return np.array(arr, copy=True)


def sgie_probe(pad: Gst.Pad, info: Gst.PadProbeInfo, ud: dict):  # noqa: C901 – complex
    buf = info.get_buffer()
    if not buf:
        return Gst.PadProbeReturn.OK

    db: FaceDB = ud["db"]
    obj_ctx = ud["obj_ctx"]
    jpeg_w = jpeg_h = int(ud.get("out_size", 112))

    batch = pyds.gst_buffer_get_nvds_batch_meta(hash(buf))
    if not batch:
        return Gst.PadProbeReturn.OK

    # pass-1: schedule GPU JPEG for every object + grab embedding
    embeddings: dict[int, tuple[np.ndarray, pyds.NvDsFrameMeta]] = {}
    l_frame = batch.frame_meta_list
    while l_frame:
        fmeta = pyds.NvDsFrameMeta.cast(l_frame.data)
        l_obj = fmeta.obj_meta_list
        while l_obj:
            ometa = pyds.NvDsObjectMeta.cast(l_obj.data)
            # tensor meta → embedding
            vec = None
            l_user = ometa.obj_user_meta_list
            while l_user:
                um = pyds.NvDsUserMeta.cast(l_user.data)
                if um and um.base_meta.meta_type == pyds.NvDsMetaType.NVDSINFER_TENSOR_OUTPUT_META:
                    tmeta = pyds.NvDsInferTensorMeta.cast(um.user_meta_data)
                    if tmeta and tmeta.num_output_layers:
                        vec = np_from_layer(pyds.get_nvds_LayerInfo(tmeta, 0))
                        vnorm = np.linalg.norm(vec) + 1e-6
                        vec /= vnorm
                        break
                l_user = l_user.next
            if vec is None:
                l_obj = l_obj.next; continue
            embeddings[int(pyds.get_ptr(ometa))] = (vec, fmeta)
            # GPU JPEG – already scaled 112×112
            args = pyds.NvDsObjEncUsrArgs()
            args.saveImg = False
            args.attachUsrMeta = True
            args.scaleImg = True
            args.quality = 95
            args.scaledWidth = jpeg_w
            args.scaledHeight = jpeg_h
            pyds.nvds_obj_enc_process(obj_ctx, args, hash(buf), ometa, fmeta)
            l_obj = l_obj.next
        l_frame = l_frame.next

    pyds.nvds_obj_enc_finish(obj_ctx)

    # pass-2: read JPEG meta + update DB
    l_frame = batch.frame_meta_list
    while l_frame:
        fmeta = pyds.NvDsFrameMeta.cast(l_frame.data)
        l_obj = fmeta.obj_meta_list
        while l_obj:
            ometa = pyds.NvDsObjectMeta.cast(l_obj.data)
            key = int(pyds.get_ptr(ometa))
            tup = embeddings.get(key)
            if tup is None:
                l_obj = l_obj.next; continue
            vec, fref = tup
            jpeg_bytes = None
            l_user = ometa.obj_user_meta_list
            while l_user:
                um = pyds.NvDsUserMeta.cast(l_user.data)
                if um and um.base_meta.meta_type == pyds.NvDsMetaType.NVDS_CROP_IMAGE_META:
                    outp = pyds.NvDsObjEncOutParams.cast(um.user_meta_data)
                    arr = outp.outBuffer()
                    if arr is not None and getattr(arr, "size", 0):
                        jpeg_bytes = bytes(arr)
                        break
                l_user = l_user.next
            meta = {
                "frame_num": int(fmeta.frame_num),
                "ntp_ts": int(getattr(fmeta, "ntp_timestamp", 0)),
                "bbox": [int(ometa.rect_params.left), int(ometa.rect_params.top),
                          int(ometa.rect_params.width), int(ometa.rect_params.height)],
                "source_id": int(getattr(fmeta, "source_id", 0)),
                "object_id": int(getattr(ometa, "object_id", -1)),
            }
            db.add_or_update(vec, jpeg_bytes, meta)
            l_obj = l_obj.next
        l_frame = l_frame.next
    return Gst.PadProbeReturn.OK

# ----------------------------------------------------------------------------
# Pipeline builder (URI agnostic)
# ----------------------------------------------------------------------------

def build_pipeline(uri: str, *, width: int, height: int, gpu: int, timeout: int, db: FaceDB,
                    out_size: int, verbose: bool) -> Gst.Pipeline:
    p = Gst.Pipeline.new("facepipe")
    u = urlparse(uri)

    # --- source branch -------------------------------------------------------
    if u.scheme and u.scheme.lower().startswith("rtsp"):
        rtspsrc = make("rtspsrc", "src", location=uri, latency=200, drop_on_latency=True)
        depay = make("rtph264depay", "depay")
        h264parse = make("h264parse", "h264parse")
        dec = make("nvv4l2decoder", "dec")
        src_q = make("queue", "qsrc", max_size_buffers=0, max_size_bytes=0, max_size_time=0)
        mux = make("nvstreammux", "mux", batch_size=1, width=width, height=height,
                   live_source=True, batched_push_timeout=timeout, sync_inputs=False)
        for el in (rtspsrc, depay, h264parse, dec, src_q, mux): p.add(el)

        def on_pad(src, pad):
            if not pad.has_current_caps():
                return
            caps = pad.get_current_caps().to_string()
            if "application/x-rtp" in caps:
                pad.link(depay.get_static_pad("sink"))
        rtspsrc.connect("pad-added", on_pad)
        link_many(depay, h264parse, dec, src_q)
        mux_sink = mux.request_pad_simple("sink_0")
        src_q.get_static_pad("src").link(mux_sink)
    else:  # treat as local file/HTTP progressive download
        src = make("filesrc", "src", location=uri)
        demux = make("qtdemux", "demux")
        h264parse = make("h264parse", "h264parse")
        dec = make("nvv4l2decoder", "dec")
        src_q = make("queue", "qsrc", max_size_buffers=0, max_size_bytes=0, max_size_time=0, flush_on_eos=False)
        mux = make("nvstreammux", "mux", batch_size=1, width=width, height=height,
                   live_source=False, batched_push_timeout=timeout)
        for el in (src, demux, h264parse, dec, src_q, mux): p.add(el)
        demux.connect("pad-added", lambda d, pad: pad.link(h264parse.get_static_pad("sink")))
        link_many(src, demux, h264parse, dec, src_q)
        src_q.get_static_pad("src").link(mux.request_pad_simple("sink_0"))

    # --- inference branch ----------------------------------------------------
    pgie = make("nvinfer", "pgie", config_file_path="/opt/models/yolov10_face/pgie.txt")
    sgie = make("nvinfer", "sgie", config_file_path="/opt/models/recognition/sgie.txt",
                process_mode=2, batch_size=256)  # large enough to avoid known freeze
    # The queue absorbs burst when many faces per frame
    post_q = make("queue", "qpost", max_size_buffers=64)
    sink = make("fakesink", "sink", sync=False)
    for el in (pgie, sgie, post_q, sink): p.add(el)

    # GPU ownership
    for el in (mux, pgie, sgie):
        el.set_property("gpu-id", gpu)

    link_many(mux, pgie, sgie, post_q, sink)

    # attach probe after SGIE (NV12)
    sgie.get_static_pad("src").add_probe(Gst.PadProbeType.BUFFER, sgie_probe,
                                          {"db": db, "obj_ctx": pyds.nvds_obj_enc_create_context(gpu),
                                           "out_size": out_size, "verbose": verbose})
    return p

# ----------------------------------------------------------------------------
# main entry
# ----------------------------------------------------------------------------

def main(argv: list[str] | None = None):
    parser = argparse.ArgumentParser(description="GPU face identification pipeline (DeepStream)")
    parser.add_argument("video", help="Path / RTSP URI to video file/stream")
    parser.add_argument("--width", type=int, default=1280)
    parser.add_argument("--height", type=int, default=720)
    parser.add_argument("--gpu", type=int, default=0, help="GPU index")
    parser.add_argument("--timeout", type=int, default=40000,
                        help="nvstreammux batched_push_timeout (µs)")
    parser.add_argument("--out_dir", default="faces_out")
    parser.add_argument("--json", default=None, help="Path to faces.json (default: <out_dir>/faces.json)")
    parser.add_argument("--sim", type=float, default=0.5, help="Cosine similarity threshold")
    parser.add_argument("--save_every", type=int, default=60, help="#frames between two snapshots")
    parser.add_argument("--size", type=int, default=112, help="Crop size (square)")
    parser.add_argument("--verbose", action="store_true")
    args = parser.parse_args(argv)

    Gst.init(None)
    json_path = Path(args.json) if args.json else Path(args.out_dir) / "faces.json"
    db = FaceDB(Path(args.out_dir), json_path, sim_thresh=args.sim, save_interval_frames=args.save_every)

    pipe = build_pipeline(args.video, width=args.width, height=args.height, gpu=args.gpu,
                          timeout=args.timeout, db=db, out_size=args.size, verbose=args.verbose)
    bus = pipe.get_bus(); bus.add_signal_watch()
    loop = GLib.MainLoop()

    def on_msg(_, msg: Gst.Message):
        t = msg.type
        if t == Gst.MessageType.ERROR:
            err, dbg = msg.parse_error(); print("ERROR:", err, dbg, file=sys.stderr)
            loop.quit()
        elif t == Gst.MessageType.EOS:
            # don’t quit immediately – wait a tick to flush remaining pads
            GLib.timeout_add(10, lambda: (loop.quit(), False)[1])
        return True

    bus.connect("message", on_msg)
    pipe.set_state(Gst.State.PLAYING)
    try:
        loop.run()
    finally:
        pipe.set_state(Gst.State.NULL)
    return 0

if __name__ == "__main__":
    sys.exit(main())
